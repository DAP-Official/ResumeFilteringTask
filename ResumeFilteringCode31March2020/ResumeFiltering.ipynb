{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Functionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate word2vec file from glove files:(Run only once, once the word2vec file is generated,we can use that directly)\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "\n",
    "glove_input_file = 'Dataset/glove.6B.100d.txt'\n",
    "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
    "\n",
    "if not os.path.exists(word2vec_output_file):\n",
    "    glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load word2vec embedded file:\n",
    "word2vec_output_file = 'Dataset/glove.6B.100d.txt.word2vec'\n",
    "embed = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# PDF to txt: (Resumes)\n",
    "\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def convertPDFToText(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    string = retstr.getvalue()\n",
    "    retstr.close()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete Approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/pratikcr7/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nazli', 'Uzgur', 'Education', 'May', 'Bachelor', 'Science', 'Computer', 'Science', 'Carnegie', 'Mellon', 'University', 'Pittsburgh', 'Relevant', 'Coursework', 'Great', 'Theoretical', 'Ideas', 'Computer', 'Science', 'Principles', 'Functional', 'Programming', 'Introduction', 'Computer', 'Systems', 'Fundamentals', 'Programming', 'Computer', 'Science', 'Principles', 'Imperative', 'Computation', 'Concepts', 'Mathematics', 'May', 'Game', 'Design', 'Minor', 'Carnegie', 'Mellon', 'University', 'Pittsburgh', 'Skills', 'Programming', 'Languages', 'Limited', 'Proficieny', 'Python', 'Java', 'SML', 'Spoken', 'Languages', 'Fluent', 'Turkish', 'English', 'Conversational', 'Proficiency', 'Japanese', 'Experience', 'NLP', 'Been', 'working', 'Natural', 'Language', 'Processing', 'program', 'January', 'Mecidiyekoy', 'EnglishTurkish', 'translator', 'company', 'June', 'Karate', 'Cofounder', 'president', 'team', 'Oct', 'Anime', 'Manga', 'President', 'club', 'Oct', 'PR', 'Public', 'relations', 'tour', 'guide', 'translator', 'Robert', 'College', 'Oct', 'Interests', 'Buggy', 'Driving', 'small', 'vehicle', 'created', 'CMU', 'students', 'competing', 'team', 'Japanese', 'Learning', 'Japanese', 'Dancing', 'Latin', 'contemporary', 'jazz', 'dance', 'years', 'Skiing', 'Skiing', 'years', 'competed', 'races', 'Volunteering', 'Peer', 'tutor', 'introduced', 'new', 'cultures', 'Turkish', 'children', 'introduced', 'children', 'movie', 'theaters', 'helped', 'international', 'orientation', 'registering', 'new', 'students', 'Selale', 'Evleri', 'Duden', 'Bahcesehir', 'Istanbul', 'Turkey', 'nazliuzgurgmailcom']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the resume:\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load the document\n",
    "filename = 'Dataset/resumes/tech/train_resume1.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311\n",
      "[('FL', 6), ('Computer', 5), ('Ocala', 5), ('Science', 4), ('years', 4), ('children', 4), ('Team', 4), ('May', 3), ('Programming', 3), ('Languages', 3), ('Japanese', 3), ('team', 3), ('Oct', 3), ('President', 3), ('College', 3), ('students', 3), ('Vanguard', 3), ('High', 3), ('School', 3), ('ages', 3), ('families', 3), ('holiday', 3), ('Carnegie', 2), ('Mellon', 2), ('University', 2), ('Pittsburgh', 2), ('Principles', 2), ('Skills', 2), ('Fluent', 2), ('Turkish', 2), ('translator', 2), ('June', 2), ('Skiing', 2), ('introduced', 2), ('new', 2), ('Smith', 2), ('Northampton', 2), ('MA', 2), ('National', 2), ('Honor', 2), ('Spanish', 2), ('EXPERIENCE', 2), ('Softball', 2), ('Summers', 2), ('sessions', 2), ('including', 2), ('Designed', 2), ('progress', 2), ('parents', 2), ('Anchor', 2)]\n",
      "311\n"
     ]
    }
   ],
   "source": [
    "# Build Vocabulary from the training and testing data:\n",
    "\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\ttokens = [word for word in tokens if word.isalpha()]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\ttokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\n",
    "# load doc and add to vocab\n",
    "def add_doc_to_vocab(filename, vocab):\n",
    "\t# load doc\n",
    "\tdoc = load_doc(filename)\n",
    "\t# clean doc\n",
    "\ttokens = clean_doc(doc)\n",
    "\t# update counts\n",
    "\tvocab.update(tokens)\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif is_trian and filename.startswith('test'):\n",
    "\t\t\tcontinue\n",
    "\t\tif not is_trian and not filename.startswith('test'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# add doc to vocab\n",
    "\t\tadd_doc_to_vocab(path, vocab)\n",
    "\n",
    "# define vocab\n",
    "vocab = Counter()\n",
    "# add all docs to vocab\n",
    "process_docs('Dataset/resumes/tech/', vocab, True)\n",
    "process_docs('Dataset/resumes/nontech/', vocab, True)\n",
    "# print the size of the vocab\n",
    "print(len(vocab))\n",
    "# print the top words in the vocab\n",
    "print(vocab.most_common(50))\n",
    "\n",
    "# keep tokens with a min occurrence\n",
    "min_occurance = 1\n",
    "tokens = [k for k,c in vocab.items() if c >= min_occurance]\n",
    "print(len(tokens))\n",
    "\n",
    "# save list to file\n",
    "def save_list(lines, filename):\n",
    "\t# convert lines to a single blob of text\n",
    "\tdata = '\\n'.join(lines)\n",
    "\t# open file\n",
    "\tfile = open(filename, 'w')\n",
    "\t# write text\n",
    "\tfile.write(data)\n",
    "\t# close file\n",
    "\tfile.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training sentences: 2\n",
      "Vocabulary size: 51\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec training:\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# load training data\n",
    "positive_docs = process_docs('Dataset/resumes/tech/', vocab, True)\n",
    "negative_docs = process_docs('Dataset/resumes/nontech/', vocab, True)\n",
    "sentences = negative_docs + positive_docs\n",
    "print('Total training sentences: %d' % len(sentences))\n",
    " \n",
    "# train word2vec model\n",
    "model = Word2Vec(sentences, size=100, window=5, workers=8, min_count=1)\n",
    "# summarize vocabulary size in model\n",
    "words = list(model.wv.vocab)\n",
    "print('Vocabulary size: %d' % len(words))\n",
    " \n",
    "# save model in ASCII (word2vec) format\n",
    "filename = 'embedding_word2vec.txt'\n",
    "model.wv.save_word2vec_format(filename, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 263, 100)          29700     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 259, 128)          64128     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 129, 128)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 16512)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 16513     \n",
      "=================================================================\n",
      "Total params: 110,341\n",
      "Trainable params: 80,641\n",
      "Non-trainable params: 29,700\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      " - 0s - loss: 8.0590 - accuracy: 0.5000\n",
      "Epoch 2/10\n",
      " - 0s - loss: 8.0590 - accuracy: 0.5000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 8.0590 - accuracy: 0.5000\n",
      "Epoch 4/10\n",
      " - 0s - loss: 8.0590 - accuracy: 0.5000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 8.0590 - accuracy: 0.5000\n",
      "Epoch 6/10\n",
      " - 0s - loss: 8.0590 - accuracy: 0.5000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 8.0590 - accuracy: 0.5000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 8.0590 - accuracy: 0.5000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 8.0590 - accuracy: 0.5000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 8.0590 - accuracy: 0.5000\n",
      "Test Accuracy: 50.000000\n"
     ]
    }
   ],
   "source": [
    "# Binary CNN Classifier for Tech and NonTech Resumes:\n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc, vocab):\n",
    "\t# split into tokens by white space\n",
    "\ttokens = doc.split()\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# filter out tokens not in vocab\n",
    "\ttokens = [w for w in tokens if w in vocab]\n",
    "\ttokens = ' '.join(tokens)\n",
    "\treturn tokens\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, vocab, is_trian):\n",
    "\tdocuments = list()\n",
    "\t# walk through all files in the folder\n",
    "\tfor filename in listdir(directory):\n",
    "\t\t# skip any reviews in the test set\n",
    "\t\tif is_trian and filename.startswith('test'):\n",
    "\t\t\tcontinue\n",
    "\t\tif not is_trian and not filename.startswith('test'):\n",
    "\t\t\tcontinue\n",
    "\t\t# create the full path of the file to open\n",
    "\t\tpath = directory + '/' + filename\n",
    "\t\t# load the doc\n",
    "\t\tdoc = load_doc(path)\n",
    "\t\t# clean doc\n",
    "\t\ttokens = clean_doc(doc, vocab)\n",
    "\t\t# add to list\n",
    "\t\tdocuments.append(tokens)\n",
    "\treturn documents\n",
    "\n",
    "# load embedding as a dict\n",
    "def load_embedding(filename):\n",
    "\t# load embedding into memory, skip first line\n",
    "\tfile = open(filename,'r')\n",
    "\tlines = file.readlines()[1:]\n",
    "\tfile.close()\n",
    "\t# create a map of words to vectors\n",
    "\tembedding = dict()\n",
    "\tfor line in lines:\n",
    "\t\tparts = line.split()\n",
    "\t\t# key is string word, value is numpy array for vector\n",
    "\t\tembedding[parts[0]] = asarray(parts[1:], dtype='float32')\n",
    "\treturn embedding\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab):\n",
    "\t# total vocabulary size plus 0 for unknown words\n",
    "\tvocab_size = len(vocab) + 1\n",
    "\t# define weight matrix dimensions with all 0\n",
    "\tweight_matrix = zeros((vocab_size, 100))\n",
    "\t# step vocab, store vectors using the Tokenizer's integer mapping\n",
    "\tfor word, i in vocab.items():\n",
    "\t\tweight_matrix[i] = embedding.get(word)\n",
    "\treturn weight_matrix\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "# To remove duplicates convert the list into a set.\n",
    "vocab = set(vocab)\n",
    "\n",
    "# load all training reviews\n",
    "tech_docs = process_docs('Dataset/resumes/tech/', vocab, True)\n",
    "nontech_docs = process_docs('Dataset/resumes/nontech/', vocab, True)\n",
    "train_docs = tech_docs + nontech_docs\n",
    "\n",
    "# create the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "tokenizer.fit_on_texts(train_docs)\n",
    "\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(train_docs)\n",
    "# pad sequences\n",
    "max_length = max([len(s.split()) for s in train_docs])\n",
    "Xtrain = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define training labels\n",
    "num_samples_train_tech = 1\n",
    "num_samples_train_nontech = 1\n",
    "num_samples_test_tech = 1\n",
    "num_samples_test_nontech = 1\n",
    "ytrain = array([0 for _ in range(num_samples_train_tech)] + [1 for _ in range(num_samples_train_nontech)])\n",
    "\n",
    "# load all test reviews\n",
    "tech_docs = process_docs('Dataset/resumes/tech/', vocab, False)\n",
    "nontech_docs = process_docs('Dataset/resumes/nontech/', vocab, False)\n",
    "test_docs = tech_docs + nontech_docs\n",
    "# sequence encode\n",
    "encoded_docs = tokenizer.texts_to_sequences(test_docs)\n",
    "# pad sequences\n",
    "Xtest = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "# define test labels\n",
    "ytest = array([0 for _ in range(num_samples_test_tech)] + [1 for _ in range(num_samples_test_nontech)])\n",
    "\n",
    "# define vocabulary size (largest integer value)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# load embedding from file\n",
    "raw_embedding = load_embedding('embedding_word2vec.txt')\n",
    "# get vectors in the right order\n",
    "embedding_vectors = get_weight_matrix(raw_embedding, tokenizer.word_index)\n",
    "# create the embedding layer\n",
    "embedding_layer = Embedding(vocab_size, 100, weights=[embedding_vectors], input_length=max_length, trainable=False)\n",
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "# compile network\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit network\n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2)\n",
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
